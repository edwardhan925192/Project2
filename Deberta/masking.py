# -*- coding: utf-8 -*-
"""Masking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lc_dj2Qhnfed_ywzrdmWb1lm0w3L_1LJ
"""

!pip install transformers
!pip install --upgrade torch transformers
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize #Nltk
from transformers import BertTokenizer #Bert
from transformers import RobertaTokenizer, RobertaModel
from transformers import DebertaTokenizer, DebertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score

import random

def masking(sentence):
    # Keywords to mask
    keywords = ["sorcerer", "wizard"]

    # Split the sentence into words
    words = sentence.split()

    # For each word...
    for i, word in enumerate(words):
        # If it's in the keywords...
        if word.lower() in keywords:
            # 15% of the time, replace it with a unique mask token
            if random.random() < 0.15:
                words[i] = f"[MASK_{word.upper()}]"

    # Join the words back into a sentence
    new_sentence = " ".join(words)

    return new_sentence