# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lc_dj2Qhnfed_ywzrdmWb1lm0w3L_1LJ
"""

from transformers import DebertaModel, AdamW, get_linear_schedule_with_warmup
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.nn import BCELoss
from sklearn.metrics import accuracy_score
import torch
class CustomDataset(Dataset):
    def __init__(self, input_ids, attention_masks,  labels=None):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        #self.ner_factors = ner_factors
        self.labels = labels

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        if self.labels is not None:
            return self.input_ids[idx], self.attention_masks[idx],  self.labels[idx]
        else:
            return self.input_ids[idx], self.attention_masks[idx]

class DebertaSequenceClassifier(nn.Module):
    def __init__(self, dropout=0.1):
        super(DebertaSequenceClassifier, self).__init__()
        self.deberta = DebertaModel.from_pretrained('microsoft/deberta-base')
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(self.deberta.config.hidden_size, 1)  # Changed to match DeBERTa output size
        self.sigmoid = nn.Sigmoid()

    def forward(self, input_ids, attention_masks):
        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_masks)
        pooled_output = outputs[0][:,0,:]
        dropout_output = self.dropout(pooled_output)

        return self.sigmoid(self.fc(dropout_output))


def encode_sentences(tokenized_sentences, tokenizer, max_length=64):
    input_ids = []
    attention_masks = []

    for tokens in tokenized_sentences:
        # Convert tokens to their IDs
        ids = tokenizer.convert_tokens_to_ids(tokens)

        # Add '<s>' and '</s>' tokens
        ids = [tokenizer.bos_token_id] + ids + [tokenizer.eos_token_id]

        # Truncate and pad sentences to max_length
        ids = ids[:max_length]
        ids += [tokenizer.pad_token_id] * (max_length - len(ids))

        # Create attention masks
        mask = [int(token_id > 0) for token_id in ids]

        # Convert lists to tensors
        ids = torch.tensor(ids).unsqueeze(0)
        mask = torch.tensor(mask).unsqueeze(0)

        input_ids.append(ids)
        attention_masks.append(mask)

    # Convert the lists into tensors
    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)

    return input_ids, attention_masks